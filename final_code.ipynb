{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Modules and Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script is made for Grad.AI for S.E.A safety competition.\n",
    "\n",
    "- Before start, please make sure you have put testing features, labels under `testfeatures`,\n",
    "  and `testlabels` folder respectively. Besides, make sure only testing file(s) (.csv) are under \n",
    "  `testfeatures` folder and only one label file (.csv) is under `testlabels` folder. \n",
    "\n",
    "- `isTesting` = False is used for local cross validation. \n",
    "   It shoulbe be set to True for testing purpose.\n",
    "\n",
    "- `fun_switches` is used to switch on/off respective feature egineering function.\n",
    "   By setting some of the switches off, this step can be made faster at a sacrifice of some predicting power.\n",
    "\n",
    "- `fun_name`, `fun_list`, ... `num_features_fun` are global variables. Please don't change them. \n",
    "\"\"\" \n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pywt\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.fftpack import fft\n",
    "from peakdetect import peakdet\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "########## Set isTesting = True for testing #################\n",
    "\n",
    "isTesting = True\n",
    "\n",
    "\n",
    "####### Set all Switches and ensemble option to True ########\n",
    "# It takes about 30 mins to run on a 400MB testing dataset \n",
    "# with all switches below turned on and the ensemble option enabled \n",
    "# \n",
    "# However, the finalised model takes about half of the time.\n",
    "\n",
    "ensemble = True\n",
    "fun_switches = [True, True, True, True, True, True] \n",
    "\n",
    "\n",
    "############ Global Variables. Don't Modify ##################\n",
    "\n",
    "fun_names = ['idf','fftf','cwt','stat','xsec','pkf']\n",
    "num_features_fun = [13, 15*10, 20*10, 15*10, 20*10, (5*3)*10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Concatenate Feature Files and Generate Helper Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def init_processing(isTesting = isTesting):\n",
    "    \"\"\"\n",
    "    Concatenate and sort data\n",
    "    \"\"\"\n",
    "    bgn = time.time()\n",
    "    \n",
    "    if isTesting:\n",
    "        folder, lbl_folder = 'testfeatures', 'testlabels'\n",
    "    else:\n",
    "        folder, lbl_folder = 'features', 'labels'\n",
    "    \n",
    "    \n",
    "    #read label file\n",
    "    yfiles = os.listdir(os.getcwd() + '/' + lbl_folder) \n",
    "    yfiles = [file for file in yfiles if file.split('.')[-1] == 'csv']\n",
    "    if len(yfiles) != 1:\n",
    "        raise Exception('Make sure {} folder has and only has .csv file as label'.format(lbl_folder))\n",
    "    yfile = yfiles[0]\n",
    "    labels = pd.read_csv(lbl_folder+'/'+yfile)\n",
    "    labels = labels[~labels['bookingID'].duplicated(keep = 'first')]\n",
    "    \n",
    "    \n",
    "    # read feature file(s)\n",
    "    files = os.listdir(os.getcwd() + '/' + folder) \n",
    "    files.sort()\n",
    "    cls = ['bookingID', 'Accuracy', 'Bearing', \n",
    "       'acceleration_x','acceleration_y', 'acceleration_z', \n",
    "       'gyro_x', 'gyro_y', 'gyro_z',\n",
    "       'second', 'Speed']\n",
    "    data  = pd.DataFrame(columns = cls)    \n",
    "    for f in files:\n",
    "        if f.split('.')[-1] == 'csv' and f != 'new.csv' and f != 'reference.csv' :\n",
    "            concat_bgn = time.time()\n",
    "            data = pd.concat([data, pd.read_csv(folder + '/' + f)])\n",
    "            concat_end = time.time()\n",
    "            print('concat',f, ' ',concat_end - concat_bgn)            \n",
    "    data = data.sort_values(by = ['bookingID', 'second'])\n",
    "    data = data.reset_index(drop = True)\n",
    "    data = data.merge(labels, how = 'left', on = 'bookingID')    \n",
    "    data.to_csv(folder+'/'+'new.csv', index = False)\n",
    "    print('Merging and sorting', str(time.time() - concat_end ))\n",
    "     \n",
    "        \n",
    "    # generate tb: a table of individual trip information  \n",
    "    tb_bgn = time.time()\n",
    "    groups = data.groupby(['bookingID'])\n",
    "    heads = groups.head(1).index\n",
    "    tails = groups.tail(1).index\n",
    "    ids = groups.head(1)['bookingID'].values.astype(int)\n",
    "    lbls = groups.head(1)['label'].values.astype(int)\n",
    "    tb = np.vstack((ids,heads,tails,lbls)).T \n",
    "    np.save(lbl_folder+'/'+'lbls.npy', lbls)\n",
    "    np.save(lbl_folder+'/'+'tb.npy', tb)\n",
    "    print('generate trip information table', time.time() - tb_bgn)\n",
    "    \n",
    "    \n",
    "    # generate reference statistics \n",
    "    if isTesting:\n",
    "        ref_data = pd.read_csv('features/reference.csv', index_col = 0)\n",
    "    else:\n",
    "        ref_bgn = time.time()\n",
    "        ref_cls = cls\n",
    "        ref_cls = [col for col in ref_cls if col != 'bookingID' and col != 'label']\n",
    "        ref_data = pd.DataFrame(columns = ref_cls, index = ['max','min','1pct','99pct'])\n",
    "        for col in ref_data.columns:\n",
    "            ref_data.loc['max', col] = np.abs(data[col]).max()\n",
    "            ref_data.loc['min', col] = np.abs(data[col]).min()\n",
    "            ref_data.loc['1pct', col] = np.percentile(data[col], [1])[0]\n",
    "            ref_data.loc['99pct', col] = np.percentile(data[col], [99])[0]  \n",
    "        ref_data.to_csv('features/reference.csv')\n",
    "        print('Generating reference statistics', time.time() - ref_bgn)\n",
    "           \n",
    "    print('Total time: ', time.time() - bgn)    \n",
    "    return data, tb, ref_data\n",
    "\n",
    "data, tb, ref_data = init_processing(isTesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#1. bookingID features\n",
    "def get_idf(trip):\n",
    "    \"\"\"\n",
    "    Return ID features based on assumption that \n",
    "    'bookingID' is generated not fully by random\n",
    "    \"\"\"\n",
    "    ID = trip['bookingID'].values[0]\n",
    "    return [int(i) for i in list('0'*13+str(ID))[-13:]]\n",
    "\n",
    "\n",
    "# 2. fast fourier transform (FFT) features\n",
    "def get_fftf(trip,  n = 10): # n is the number of low frequency components\n",
    "    \"\"\"\n",
    "    Frequency domain features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for col in trip.columns:\n",
    "        if col != 'bookingID' and col != 'label':\n",
    "            fourier = fft(trip[col]) \n",
    "            fourier = np.abs(fourier)/trip.shape[0]\n",
    "            interval = 2\n",
    "            # skip DC component, keep n low freq components at a chosen interval\n",
    "            if len(fourier) > n*interval+1:\n",
    "                lowf = fourier[np.arange(1, n*interval+1,interval)]  \n",
    "            else:\n",
    "                length = math.floor(len(fourier)/interval)\n",
    "                lowf = ([fourier[i*interval+1] for i in range(length)] + [0]*n)[:n]\n",
    "            features += list(lowf) \n",
    "            features += list(np.percentile(fourier, [75, 90, 95, 97, 100])) #  distribution stats\n",
    "    return features\n",
    "\n",
    "\n",
    "# 3.  continuous wavelet transform (CWT) features\n",
    "def get_cwt(trip, n = 20): # n is the number of scales\n",
    "    \"\"\"\n",
    "    Return time-frequency coefficients with dimension reduced PCA\n",
    "    \"\"\"\n",
    "    scales = np.arange(1, n + 1)\n",
    "    features = []\n",
    "    for col in trip.columns:\n",
    "        if col != 'bookingID' and col != 'label':\n",
    "            coef, freq = pywt.cwt(trip[col].values, scales,'morl')\n",
    "            features += list(PCA(n_components = 1).fit_transform(coef).flatten())\n",
    "    return features\n",
    "\n",
    "\n",
    "# 4. statatistics features\n",
    "def get_stat(trip, percentiles = [1, 3, 5, 10, 25, 75, 90, 95, 97, 99]):  \n",
    "    \"\"\"\n",
    "    Return time domain statistics\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for col in trip.columns:\n",
    "        if col != 'bookingID' and col != 'label':\n",
    "            s = trip[col]\n",
    "            basics = [s.mean(), s.mode().values[0], s.max(), s.min(), s.std()]            \n",
    "            dist = np.percentile(trip[col], percentiles)\n",
    "            features += basics\n",
    "            features += list(dist)\n",
    "    return features\n",
    "\n",
    "\n",
    "# 5. cross-section features in ratio\n",
    "def get_xsec(trip, n = 20): # n is the number of interval\n",
    "    \"\"\"\n",
    "    Contextual driving style in ratios by comparing \n",
    "    driving style with peer statatistics\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for col in trip.columns:\n",
    "        if col != 'bookingID' and col != 'label':\n",
    "            ceiling = ref_data.loc['99pct',col]\n",
    "            floor = ref_data.loc['1pct',col]\n",
    "            interval = 1.*(ceiling - floor)/n\n",
    "            parts = [floor+interval*i for i in range(n)]\n",
    "\n",
    "            length = trip.shape[0]        \n",
    "            cum_ratio_parts = [trip[trip[col] < part].shape[0]/length for part in parts]\n",
    "            ratio_parts = [cum_ratio_parts[0]]\n",
    "            for i in range(1,n):\n",
    "                ratio_parts.append(cum_ratio_parts[i] - cum_ratio_parts[i-1])\n",
    "            features += ratio_parts\n",
    "    return features\n",
    "\n",
    "\n",
    "# 6. peak groups features\n",
    "def cal_peaks(maxtab, mintab):\n",
    "    \"\"\"\n",
    "    Calculate STD, average intensity and frequency for given set of peaks found\n",
    "    \"\"\"\n",
    "    if len(maxtab) + len(mintab) < 2:\n",
    "        return [0,0,0]\n",
    "    peaks = np.array([i for i in zip(maxtab[:, 1], mintab[:, 1])]).flatten()\n",
    "    if len(maxtab) > len(mintab):\n",
    "        peaks = np.hstack((peaks,  [maxtab[-1,1]]))\n",
    "    duration = max((maxtab[-1,0] - maxtab[0,0]), mintab[-1,0] - maxtab[0,0])\n",
    "    ave_freq = (len(maxtab) + len(mintab))/duration\n",
    "\n",
    "    intensity = [np.abs(peaks[i+1]-peaks[i]) for i in range(len(peaks)-1)]\n",
    "    ave_intensity = np.mean(intensity)\n",
    "    std_intensity = np.std(intensity)\n",
    "    return [ave_freq, ave_intensity, std_intensity]\n",
    "\n",
    "\n",
    "def get_pkf(trip, window_size = 20, n = 5): # n is the number of interval\n",
    "    \"\"\"\n",
    "    Statistics (STD, average intensity/frequency) of \n",
    "    local maxima/minima groups that found at different threholds\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    for col in trip.columns:\n",
    "        if col != 'bookingID' and col != 'label':\n",
    "            high_ref = ref_data.loc['max', col]\n",
    "            low_ref = ref_data.loc['min', col]\n",
    "            interval = (high_ref - low_ref)/n\n",
    "            series = trip[col]\n",
    "            \n",
    "            if len(trip[col]) > window_size + 1:\n",
    "                series = savgol_filter(series, window_size + 1, 2)\n",
    "                \n",
    "            for i in range(n):\n",
    "                maxtab, mintab = peakdet(series, max(low_ref + i * interval, interval))\n",
    "                features += cal_peaks(maxtab, mintab)\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def get_FeaturesnLabels(data, tb, isTesting = isTesting):\n",
    "    \"\"\"\n",
    "    Common utility by different functions to calculate respective features\n",
    "    \"\"\"\n",
    "\n",
    "    print('Getting_features ...')  \n",
    "    report_interval = len(tb)//20\n",
    "    bgn = time.time()\n",
    "    \n",
    "    features = []    \n",
    "    for i in range(len(tb)):\n",
    "        if i%report_interval == 0:\n",
    "            print(\"progress\",str(5*i//report_interval),\"% \", str(time.time()-bgn))\n",
    "\n",
    "        ID, head, tail, _ = map(int,tb[i])\n",
    "        trip = data.iloc[head:tail,:]\n",
    "        row = [] \n",
    "        \n",
    "        for j in range(len(fun_list)):\n",
    "            fun = fun_list[j]\n",
    "            name = fun_names[j]\n",
    "            num = num_features_fun[j]\n",
    "            switch_on = fun_switches[j]            \n",
    "            if switch_on:\n",
    "                tripfeatures_byfun = fun(trip)\n",
    "                actual_num = len(tripfeatures_byfun)\n",
    "                if len(tripfeatures_byfun) != num:\n",
    "                    raise Exception('Input num of features returned by '+name+\n",
    "                                    ' is '+ str(actual_num)+' instead of '+ str(num))\n",
    "                row += tripfeatures_byfun\n",
    "            else:\n",
    "                row += [0]*num\n",
    "                \n",
    "        features.append(row)\n",
    "        \n",
    "    features = np.array(features)\n",
    "    \n",
    "    file_name = 'features_{}.npy'.format(str(sum(num_features_fun)))\n",
    "    if isTesting:\n",
    "        np.save('testfeatures/' + file_name, features)\n",
    "    else:\n",
    "        np.save('features/' + file_name, features)\n",
    "    print('Total time: ', time.time()-bgn)\n",
    "    \n",
    "    return features, tb[:,-1]\n",
    "\n",
    "fun_list = [get_idf, get_fftf, get_cwt, get_stat, get_xsec, get_pkf]\n",
    "features, labels = get_FeaturesnLabels(data, tb, isTesting = isTesting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_kfolds(n, n_folds = 5):   \n",
    "    \"\"\"\n",
    "    Generate static splits for local cross validation\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(n_splits = n_folds, random_state = 3, shuffle = True)\n",
    "    folds = list(kf.split(range(n), [0]*n))\n",
    "    return folds\n",
    "\n",
    "\n",
    "def get_estimator_preds(isTesting = isTesting):\n",
    "    \"\"\"\n",
    "    Generate predictions from 6 base estimators based on features extracted\n",
    "    \"\"\"\n",
    "    if isTesting:\n",
    "        xfolder = 'testfeatures'\n",
    "    else:\n",
    "        xfolder = 'features'\n",
    "        \n",
    "    # sanity check    \n",
    "    xfiles = os.listdir(os.getcwd() + '/' + xfolder)  \n",
    "    xfile = [file for file in xfiles if file.split('.')[-1] == 'npy']\n",
    "    if len(xfile) != 1 :\n",
    "        raise Exception('''Make sure {} folder has and only has one .npy file'''.format(xfolder))\n",
    "    xfile = xfile[0]\n",
    "               \n",
    "    estimators = [\n",
    "        GaussianNB(),\n",
    "        LogisticRegression(),\n",
    "        MLPClassifier(),\n",
    "        KNeighborsClassifier(5),\n",
    "        DecisionTreeClassifier(),\n",
    "        GradientBoostingClassifier(max_depth = 3)]\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    bgn = time.time() \n",
    "    for i in range(len(fun_names)):            \n",
    "        if fun_switches[i]:\n",
    "            round_bgn = time.time()    \n",
    "            print('Generating predictions on', fun_names[i],'features ...')\n",
    "            ed = np.cumsum(num_features_fun)[i]\n",
    "            st = ed - num_features_fun[i]\n",
    "            \n",
    "            X = np.load('features/features_{}.npy'.format(str(sum(num_features_fun))))[:,st:ed]            \n",
    "            y = np.load('labels/lbls.npy')\n",
    "            \n",
    "            folds = get_kfolds(X.shape[0], 5)            \n",
    "            for estimator in estimators:\n",
    "                idx = []\n",
    "                preds4estimator = []\n",
    "                for train_idx, test_idx in folds:\n",
    "                    idx += list(test_idx)    \n",
    "                    X_train, y_train = X[train_idx], y[train_idx]\n",
    "                    if isTesting: \n",
    "                        X_test = np.load('testfeatures/'+xfile)[:,st:ed]\n",
    "                    else:\n",
    "                        X_test = X[test_idx]\n",
    "                    partial_pred = estimator.fit(X_train, y_train).predict_proba(X_test)[:,0]\n",
    "                    preds4estimator.append(partial_pred)\n",
    "                if isTesting:\n",
    "                    preds4estimator = np.mean(np.array(preds4estimator), axis = 0)\n",
    "                else:\n",
    "                    preds4estimator = np.array(preds4estimator).flatten()[np.argsort(idx)]\n",
    "                features.append(preds4estimator)\n",
    "            round_end = time.time()   \n",
    "            print(round_end - round_bgn)\n",
    "            \n",
    "        else: # if switched off, simply write zeros\n",
    "            num_trips = np.load(xfolder+'/'+xfile).shape[0]\n",
    "            for _ in estimators:\n",
    "                features.append([0]*num_trips)\n",
    "                \n",
    "    features = np.array(features).T\n",
    "    np.save(xfolder+'/pred.npy', features)\n",
    "    print('Total time: ', time.time() - bgn)    \n",
    "    return features\n",
    "\n",
    "column_indexing = []\n",
    "pred_indexing = []\n",
    "for i in range(len(fun_names)):\n",
    "    column_indexing += [fun_switches[i]]*num_features_fun[i] \n",
    "    pred_indexing += [fun_switches[i]]*len(fun_list)\n",
    "    \n",
    "if ensemble:\n",
    "    preds = get_estimator_preds(isTesting = isTesting)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Display Model Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_display(isTesting = isTesting, fun_switches = fun_switches, ensemble = ensemble):\n",
    "    \"\"\"\n",
    "    Display model performance using AUC as the metric\n",
    "    \"\"\"\n",
    "    \n",
    "    if isTesting: # testing the model on actual testing dataset\n",
    "        num_f = sum(num_features_fun)\n",
    "        if ensemble:\n",
    "            X_train = np.load('features/pred.npy')[:,pred_indexing]\n",
    "            X_test = np.load('testfeatures/pred.npy')[:,pred_indexing]\n",
    "        else:\n",
    "            X_train = np.load('features/features_{}.npy'.format(num_f))[:,column_indexing]\n",
    "            X_test = np.load('testfeatures/features_{}.npy'.format(num_f))[:,column_indexing]\n",
    "        y_train = np.load('labels/lbls.npy')\n",
    "        y_test = labels \n",
    "\n",
    "        # For local sanity check on performance only. No need to uncomment\n",
    "        all_idx, idx = np.load('labels/tb.npy')[:,0], np.load('testlabels/tb.npy')[:,0]\n",
    "        selection = [True if ID not in idx else False for ID in all_idx]\n",
    "        X_train = X_train[selection]\n",
    "        y_train = y_train[selection]\n",
    "\n",
    "    else:# local cross validation\n",
    "        if ensemble:\n",
    "            X = np.load('features/pred.npy')[:,pred_indexing]\n",
    "        else:\n",
    "            X = np.load('features/features_{}.npy'.format(sum(num_features_fun)))[:,column_indexing]\n",
    "        y = np.load('labels/lbls.npy')\n",
    "\n",
    "        kf = StratifiedKFold(n_splits = 5, random_state = 0, shuffle = True)\n",
    "        folds = list(kf.split(X, y))\n",
    "        train_index, test_index = folds[0]\n",
    "        X_train, X_test    = X[train_index], X[test_index]\n",
    "        y_train, y_test    = y[train_index], y[test_index]  \n",
    "    y_test = y_test.astype(int)\n",
    "\n",
    "    print('If isTesting:', isTesting)\n",
    "    print('If ensemble:', ensemble)   \n",
    "    print('Function names:', fun_names)\n",
    "    print('If turned on:', fun_switches)\n",
    "    print('Data shape:', X_train.shape, y_train.shape, X_test.shape, y_test.shape, '\\n')\n",
    "\n",
    "    colors = ['grey','black','purple','red']\n",
    "    baseline_prob = [GaussianNB(),\n",
    "                     KNeighborsClassifier(5),\n",
    "                     LogisticRegression(),\n",
    "                     GradientBoostingClassifier(max_depth = 3)]\n",
    "\n",
    "    \n",
    "    rocauc_results = {}\n",
    "    for clf in baseline_prob:\n",
    "        bgn = time.time() \n",
    "        clf_name = clf.__class__.__name__\n",
    "        rocauc_results[clf_name] = {'auc':0,'fpr':[],'tpr':[]}\n",
    "\n",
    "        clf.fit(X_train,y_train)\n",
    "        rocauc_results[clf_name]['fpr'],rocauc_results [clf_name]['tpr'],_ = roc_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        rocauc_results[clf_name]['auc'] = auc(rocauc_results[clf_name]['fpr'], rocauc_results [clf_name]['tpr'])\n",
    "        end = time.time()\n",
    "        print(clf_name + \" AUC score:\",rocauc_results[clf_name]['auc'], \" Time: \" + str(end - bgn))\n",
    "\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.plot([0, 1], [0, 1], color='navy',linestyle='--')\n",
    "\n",
    "    for i,name in enumerate(rocauc_results.keys()):\n",
    "        plt.plot(rocauc_results[name]['fpr'], rocauc_results[name]['tpr'], color=colors[i],label='{} (area = {:.2f})'.format(name,rocauc_results[name]['auc']))\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    plt.xlabel('False Positive Rate');\n",
    "    plt.ylabel('True Positive Rate');\n",
    "    plt.title('ROC Curve(Baseline)');\n",
    "    plt.legend(loc=\"lower right\");  \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 All feature generated included + Ensemble enabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with all switches turned on,\n",
    "# it took around 30 minutes to model a 400MB testing dataset with MacBook Pro (2.6GHz)\n",
    "\n",
    "X_train, y_train, X_test, y_test = ROC_display(isTesting = isTesting, fun_switches = fun_switches, ensemble = ensemble)\n",
    "plt.savefig('ROC_allon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2  All feature generated included + Ensemble disabled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switching ensemble off saves 60% time with a majorly deteriorted AUC \n",
    "\n",
    "ensemble = False\n",
    "fun_switches = [True, True, True, True, True, True] \n",
    "\n",
    "column_indexing = []\n",
    "pred_indexing = []\n",
    "for i in range(len(fun_names)):\n",
    "    column_indexing += [fun_switches[i]]*num_features_fun[i] \n",
    "    pred_indexing += [fun_switches[i]]*len(fun_list) \n",
    "    \n",
    "    \n",
    "ROC_display(isTesting = isTesting, fun_switches = fun_switches, ensemble = ensemble)\n",
    "plt.savefig('ROC_ensembledisabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3  The Best Trade-off between Cost and Performance\n",
    "    \n",
    "## BookingID and Cross-Sectional Features Included + Ensemble Enabled\n",
    "     \n",
    "          \n",
    "                               \n",
    "- One-layer ensemble model built on 12 predictions that made on 213 raw features \n",
    "- Took around 15 mins running time in total for a 400MB testing dataset on MacBook Pro (2.6GHz), inclusive of concatenating, sorting, preprocessing, engineering and ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switching any function below off saves around 10% of time \n",
    "# (except for the first switch for bookingID features) with a minorly deteriorted AUC  \n",
    "                      \n",
    "ensemble = True\n",
    "fun_switches = [True, False, False, False, True, False] \n",
    "\n",
    "column_indexing = []\n",
    "pred_indexing = []\n",
    "for i in range(len(fun_names)):\n",
    "    column_indexing += [fun_switches[i]]*num_features_fun[i] \n",
    "    pred_indexing += [fun_switches[i]]*len(fun_list) \n",
    "\n",
    "    \n",
    "ROC_display(isTesting = isTesting, fun_switches = fun_switches, ensemble = ensemble)\n",
    "plt.savefig('ROC_onlyidNxsec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
